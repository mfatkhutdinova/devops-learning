Kubernetes - это технология, которая позволяет автоматизировать развертывание и масштабирование контейнеризированных приложений, а также управление ими.
Kubernetes - это менеджер контейнеров.

Преимущества Kubernetes:
  Immutable 
  Declarative
  Self-healing
  Decoupling - каждый компонент инфраструктуры независим от других 

API сервер - входная точка для запуска в Kubernetes
Мастер - смотрит в API сервер. Занимается операционными делами
Кублет - нода агент, который стоит на всех серверах кластера и отправляет инфу докер демону о том, что нужно запустить какой-то контейнер.
Воркер ноды - тут запускаются приложения. На каждой ноде есть кублет.  

Абстракции Kubernetes (YAML): .
  POD - минимальная абстракция, с которой работаей Kubernetes. Тут запускаются приложения. .
  В POD может быть несколько контейнеров. 
  1 POD = 1 контейнер 
  В любом поде будет +1 контейнер, который называется pause контейнер. Он ничего не делает. Его функция - создать сетевой namespace. 
  kubectl get pod --show-labels

  ReplicaSet - родитель POD, управляет тем, в каком кол-ве наше приложение должно запускаться. .
  Labels - метка 
  Selector - инструкция для репликасет, глядя в него понимает, какие POD его. Понимает он это благодаря метке на подах. 
  Смотрит на ко-во, но не на обновление 

  Deployment - это родитель ReplicaSet. Нужен, чтобы нормально обновлять версии приложения. С этой обстракцией бычно работают на проде .
    rollingUpdate - поочередно и постепенно удаляет и создает реплики .
    recreate - пребивает сразу все реплики и потом поднимает .
    Probes - контроль за состоянием приложения во время его жизни, старта, готово ли приложение. .
      Liveness Probe .
        контроль за состоянием приложения во время его жизни
        исполняется постоянно 
      Readiness Probe .
        проверяет, готово ли приложение принимать трафик 
        в случае неудачного выполнения приложение убирается из балансировки 
        исполняется постоянно 

    Resources - содержит лимиты и резервировать запросы. .
      Limits - кол-во ресурсов, которые POD может использовать
      Reguets - кол-во ресурсов, которые резервируются для POD на ноде (CPU, memory)
    Secret - сущность для хранения паролей, токены, сертификаты и т.д. .
    kubectl rollout undo deployment name-deployment - откатывается назад 

  ConfigMap - сущность, которая хранит настройки приложения. .
    Можно указывать про настройки монтирования, про тома.

  Service - умеет объединять разрозненные поды со случайными ip адресами объединить и каким-то образом и распределять на них трафик .
    Автоматически создается объект ENDPOINTS. Т.е. сервис создался, выбрал все поды по меткам, и адреса всех этих подов сохрани в ENDPOINTS.
  Ingress - для получения запросов с наружи. Описывает правила доступа к нашему приложению .
    IngressController - принимает трафик снаружи из интернета и на основании ингрессов определяет, куда именно его послать. 
  PV/PVC - описывает том, на котором хранятся наши данные (БД) .
    PV Provisioner - программа,  котрая умеет взаимодействовать с провайдером (который предоставляет тома с данными) и создавать на этом провайдере том с нужным размером. 
  StorageClass - манифест, в котором описывается параметры доступа к провайдеру хранения данных .
  

Компоненты кластера .
  Etcd - тут хранится вся информация о нашем кластере, все настройки .
    Etcdctl - утилита управления кластером ETCD
    Требует быстрых дисков 

  API server - центральный компонет Kubernetes .
    Единственный, кто общается с Etcd
    Работает по REST API 
    Авторизация и аутентификация 

  Controller-manager - набор контроллеров .
    Node controller - контролирует состояние нод 
    Replication controller - контролирует репликасеты
    Endpoints controller
    И другие..
    GarbageCollecor - сборщик музыка

  Scheduler - назначает PODы на нады, учитывая: .
    QoS
    Affinity/anti-Affinity - смотрит на правила, где нужно запсукать приложения
    Requested resources - учитывает лимиты, запросы, зарезервированные ресурсы

  Kubelet - работает на каждой ноде .
    Единственный компонент, работающий не в Docker 
    Отдает команды Docker daemon 
    Создает PODы

  Kube-proxy - управляет сетевыми правилами на нодах .
    Смотрит в Kube-API
    Стоит на всех серверах 
    Фактически реализует Service (ipvs и iptables) 

  Контейнеризация - docker .
  Сеть .
  DNS .

Сеть Kubernetes .
  Network plugin (Flannel, Calico) .
    Обеспечивает связь между нодами и подами 
    Раздает IP-адреса подам
    Реализует шифрование между нодами 
    Управляет Network Policies

Kubespray .
  Это сценарий для установки Kubernetes 

Продвинутые абстракции .
  DeamonSet: .
    - запускает поды на всех нодах кластера 
    - при добавлении ноды - добавляет под 
    - при удалении ноды GC удаляет под 
    - описание практически полностью соответствует Deployment  (кол-во реплик не указываем, кол-во реплик = кол-во узлов в кластере)
    - tolerations - зараза и сопротивляемость 
  
  StatefulSet .
    - позволяет запускать группу подов (как Deployment)
    - гарантирует их уникальность
    - гарантирует их последовательность
    - PVC template
    - при удалении не удаляет PVC 
    - используется для запуска приложений с сохранением состояния 
      - Rabbit 
      - DBs
      - Redis
      - Kafka 
  
  InitContainers .
    - позволяет выполнить настройки перед запуском основного приложения
    - выполняются по порядку описания в манифесте
    - можно монтировать те же тома, что и в основных контейнерах 
    - можно щапускать от другого пользователя 
    - должен выполнить дейтсвие и остановиться

  Headless Service .
    - spec.clusterIP: None
    - резолвится в IP всех экдпоинтов
    - создает записи с именами всех эндпоинтов


  Job .
    - создает POD для выполнения задачи (обычно испоьзуют для единоразового использвания (env настроить))
    - перезапускает PODы до успешного выполнения задачи
    - или истечения таймаутов (activeDeadLineSeconds, backoffLimit) 

  CronJob .
    - создает Job по расписанию  (cron формат)
    - важные параметры:
      - startingDeadlineSeconds - стремная опция. Работает не так, как описано в доке. Никогда лучше не испоьзовать.
      - concurrencyPolicy - Allow/Forbit запуск экземпляов в одно и тоже время. Использовать Forbit
      - successfulJobsHistoryLimit - кол-во успешных выполненных задач сохраняем 
      - failedJobsHistoryLimit - кол-во неудачных выполненных задач сохраняем

  Role Based Access Control (RBAC) .
    - ServiceAccount - (в Kubernetes нет юзеров) сущность придумана для того, чтобы приложения Kubernetes работали под ServiceAccount
    - Role - описывает, что можно делатьв разделе Kubernetes (правила)
    - RoleBinding - в сущности указывается связь ServiceAccount с Role
    - ClusterRole - то же самое, что и Role, толко действует на весь кластер 
    - ClusterRoleBinding - то же самое, что и RoleBinding, толко действует на весь кластер

  Задача мониторинга: .
  - на каждой ноде автоматически запускается агент 
  - управляются агенты из одной точки
  - конфигурируются так же из одной точки 

Публикация приложений .
  Kubernetes Service: .
    - ClusterIP - настройка взаимодействий PODов внутри. Не внешне 
    - NodePort - публикация внешне (порты от 3000 до 32000)
    - LoadBanalcer - публикация внешне, но в облаках 
    - ExternalName - можно перенаправлять запросы на другие сервисы (почти не используется)
    - ExternalIPs - при создании сервиса на всех серверах кластера создаются по аналогии с Service type: Node Port правило трансляции, 
      который говорит, что трафик, приходящий на адресс ExternalIPs надо отправлять в PODы. Эти правила будут создавны на всех серверах.
      Это решает админ кластера с задачей балансера. 
    - Headless - none 

  Ingress .
    - IngressController - работает на принципе балансера 
    - манифест, в котором описано правило для IngressController 
    - annotations - возможность передать какую-то специфическую информацию контроллеру или оператору (то, что не влезает в манифест)

  Cert-manager .
    - начинался как способ получить сертификат от LetsEncrypt
    - автоматизирует получение SSL/TLS-серификатов от различных удостовуряющих центров (LetsEncrypt)
    - интергрируется с IngressController
    - автоматизирует продление сертификатов 

NameSpace .
  - возможность разделить окружение 
  - используется для разделения команд, проектов и русорсов кластера 
  - два одинаковы объекта с одинаковыми именами не могут быть созданы в одном namespace 
  - ресурсы задаются в рамках namespace через ResourceQuota 

Helm .
  - хорош тем, что если при релизе что-то упало, то сразу откатывается назад. И не надо никуда лезть 
  - темпейтирует приложение 
  - пакетный менеджер 
  - декларативный 
  - CNCF 
  - состоит из Helm + Tiller в (v2)
  - система плагинов 
  - есть важные фичи для построения CD 
    - watch 
    - rollback 

  Пакет у Helm: .
  - набор теймпетированных манифестов 
  - файл со значениями переменных 
  - мета 
  все это архивированно в .tgz, который называется чартами 
  В values.yml задаются все переменные, что можно гибко использовать в Deployment.

  Тестирование релиза .
    1. Создаем папку templates/tests 
    2. Кладем туда манифесты объектов k8s, которые будут тестить рлиз 
    3. Манифесты должны содержать аннотацию helm.sh/hook : test 
    4. Запускаем в CI helm test <release name>

Хранение данных на примере ceph .
  Подключение к подам SC/PVC/PV .
    Storage class - хранит параметры подключения 
    PersistentVolumeClaim - описывает требования к тому 
    PersistentVolume - хранит параметры и стату тома 
    Provisioner - параметр SC, плагин создания томов 

Disaster recovery Kubernetes = аварийное восстановление Kubernetes .
  Как можно сломать kubernetes? .
    - серверы с k8s сгорели, уборщица выдернула кабель
    - залезли руками в логику k8s, в etcd 
    - джуниор удалил namespace kube-system
    - протухли kubernetes-сертификаты
    - криво обновили версию кластера

  На что нужно обратить внимание? .
    - отказоустойчивость сетап kubernetes
        кол-во компонентов = кол-ву etcd мастеровых 
        Kubelet, kubeproxy через nginx 

    - резервирование ingress-controller'a 

    - сертификаты кластера
        это те, что лежат в /etc/kubernetes/pki 
        в старых версиях kubernetes сертификаты протухли 
        пользуйтесь нашим форком Kubespray: https://github.com/southbridgeio/kubespray
        с версии 1.15 стало легче дить, а с 1.17 совсем легко

    - регламентные работы (обновление кластера или ПО)
        убеждаемся, что у нашего приложения >1 реплик
        эвакуируем поды с нужной ноды (kubectl drain)
        проводим обновдение
        снимаем ноду с обслуживания (kubectl uncordon)

    - обновление версии кластера:
        пробуем на dev-среде/стенде и читаем Changelog!
        обновляем основную мастер-ноуды на 1 версию
        обновляем остальные мастер-ноды на 1 версию
        в kubespray для этого есть отдельный сценарий 

    - бэкапы
        что бэкапим?
        манифесты
        секреты
        сертификаты и настройки узлов
        образы контейнеров
        содержимое value 
        БД 
        можно использвать утилиту Hepito Velero

   - расширенные инструменты k8s (PSP, PDB, RQ..)
  
  Полезные штуки .
    1. PodDisruptionBudgets - ограничиваем кол-во недоступных инстансов приложения
    2. Pod Security Policy - контролируем возможности подов (privileged, hostNetwork)
    3. PriorityClass - приоритизуем более важные поды над менее важными. MUST HAVE!
    4. LimitRange - ставим дефолтные лимиты на ресурсы подов (CPU, RAM) MUST HAVE!
    5. ResourceQuota - ограничиваем ресурсы в namespace и кол-во объектов (подов, сикретов)
    6. Network Policy - "файервол" внутри кубернетес   

  Выводы: .
    - иметь описанный регламент по типовым работам 
    - размещаться в хорошем ДЦ
    - настроить отказоустойчивый сетап 
    - иметь dev-стенд, где экспериментировать 
    - покрыться мониторингами, делать бэкапы 
    - держать руку на пульсе, иметь актуальные знания 
    - пользоваться всеми фишками кубернетес
    
    
Обновление кластера Kubernetes .
  Проблемы при обновлении .
    1. устаревшие версии манифестов 
    2. устаревшие ключи запуска 
    3. измененные опции 
    4. исправленные ошибки 
    5. обновление docker 

  Порядок обновления .
    1. изучаем Change log
    2. устанавливаем тестовый кластер 
    3. обновляем тестовый кластер 
    4. деплоим приложения, проверяем работу
    5. планируем время обновления 
    6. делаем бэкапы 
    7. обновляем прод по одной ноде 
    8. после каждой ноды проверяем работоспособность 

  Что обновляем .
    - etcd database
    - control plane: API, controller-manager, scheduler + kubelet 
    - kubelet on worker proxy 
    - kube-proxy 
    - kube-flannel 
    - coredns, nodelocaldns
    - ingress-nginx-controller
    - certificate 

Траблшутинг кластера. Решения проблем при эксплуатации .
  В этой лекции много практики. Было много ошибок при настройке k8s, лектор поочередно все исправляет и показывает, на что нужно обратить внимание. 
  "если что-то не понятно, давайте удалим" - гениальная фраза!

Мониторинг кластера kubernetes .
  В Prometheus не нужно хранить долго данные. В TSDB хранят буквально несколько часов.
  Алертинги настраиваюися на основе label.
  Таргеты (цели) - мы пишем, что нужно искать.

  Все запросы пишутся на языке PromQL.

Логирование в kubernetes .
  Как там у Docker? .
    - есть несколько лог-драйверов (плагинов)
    - логи приложение должно писать в stdout/stderr
    - настройка логирования производятся в deamon.json
    - Docker CE имеет ограниченный набор плагинов по сранение с Docker EE 

  Docker работает в поде. Через stdout/stderr пишутся логи в log-file.log. Агент сборщик забирает данные из log-file.log и отправляет в log backend.
  Особенности логирования k8s: .
    - сохранять между деплоями
    - агрегировать со всех инстансов
    - добавлять мету 
    - парсить 
    - на текущий момент нет никакого стандарта 

  Grafana Loki .
    - легко устанавливается 
    - потребляет мало ресурсов 
    - не требует доп ПО 
    - используется для хранения TSDB
    - для визуализации используется Grafana 

  Язык LogQL. 
  
  Elastic + Fluent-bit + Kibana = EFK 
  Fluent-bit потребляет меньше ресурсов (в 40 раз), чем FluentD 
  Fluentd собирает логи со всех подов, все эти логи он отправляет в хранилище (elastcksearch, postgre, kafka), kibana достает все эти логи их хранилища и визуализирует.

Требования к разработке приложения в Kubernetes .
  Логи .
    Куда: stdout/stderr ; сборщик логов ; не в файлик! 
    Формат: json (идеально) ; чтобы можно было парсить ; избегать многострочных 

  Конфигурация .
    ENV переменные 
    Конфиги: удобный формат, используем configmap (yml)
    Секретную информацию - в секреты  

  Health check .
    Проверка того, что ваше приложение работает 
    Readiness/Liveness пробы 
    Проверять нижележащие сервисы 
    Статус ответа: 2xx - успешно, 5xx - ошибка 
    
  Graceful Shutdown .
    Корректное (мягкое) завершение работы:
      - учитываем завершение операций (считаем, что для приложения нужно 30 сек для завершения всех операций на очереди, после 45 сек принудительно кильнуть)
      - учитываем завершение сессий 
      - возвращаем адекватный exit-код 
      - учитываем на клиенте (сессии могут порваться и их нужно переустановить)

  Ресурсы .
    РЕСУРСЫ НУЖНО ПРОСТАВЛЯТЬ!!!
    Requestы и лимиты 

  Хранение данных .
    В идеале данные не храним 

  Cloudnativeness .
    Можно использовать возможности Kubernetes
      - запускать воркеры
      - определять эндпоинты 

    Использовать возможности облака 
      - балансировка 
      - масштабирование 
    
  Infrastructure as a code .
   
Докеризация и CI/CD .
  1. Докеризация решает проблемы повторяемости 
  2. Позволяет значительно упростить и ускорить процесс доставки новых изменений в тестовую среду и в продакшен.
  3. При использовании автоматизированного тестирования позволяет отсеять некоторый процент ошибок на пути в продакшен.
  4. Позволяет хранить историю сборок за какой-то промежуток времени, при необходимости можно откатиться назад.

  Возможные проблемы: 
    1. ваше приложение читает настройки из файлов (нужно в переменные окружения)
    2. приложение пишет в логи только в файл (нужно в stdout/stderr)
    3. приложение сохраняет важные данные в файлы во время работы (нужно использовать внутренние хранилища)
    4. приложению нужны права root (нужно, чтобы не нужны были права root)
    5. нужно четко понимать, какие версии ПО нужны для запука приложения и указывать это в Dockerfile 
    6. нужно хотя бы примерно понимать, какие аппаратные ресурсы нужны приловению для работы 

  Автоматизация труда Gitlab:
    - система контроля версий
    - встроенный Docker registry 
    - разделение инстансов - gitlab и gitlab-runner 
    - поддержка Kubernetes и Auto DevOps 

  Этапы CI/CD .
    1. build 
    2. test
    3. cleanup 
    4. push 
    5. deploy 

Observability - принципы и техники наблюдения за системой .
  Мониторинг .
    Blackbox мониторинг - закрытая коробка, она у нас есть и она не сломалась  
    Whitebox мониторинг - смотрим внутрь 

    Что алертить?
      - то, что требует действий 
      - обязательно разделение по важности
      - все остальное решает визуализация и регламент ее обработки 
  
  Логи .
    Логи - это события 
    Парсить: Fluent[d|bit]; Logstash 
    Хранить: ElastckSearch; ClickHouse; Loki
    Строить визуализацию: Grafana(можно интегрировать с метриками); Kibana 
    Алертить: Kibana; Grafana; Elastalert

  Трейсинг .
    Поиск узких мест 
    Визуализация пути выполнения работы 
    Сбор аналитической информации 

  Application Performance Monitoring .
    Мониторинг + Логи + Трейсинг 