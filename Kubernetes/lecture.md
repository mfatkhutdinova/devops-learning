# Kubernetes

Kubernetes — это технология, которая позволяет автоматизировать развертывание и масштабирование контейнеризированных приложений, а также управление ими.  
Kubernetes — это менеджер контейнеров.

## Преимущества Kubernetes

- Immutable  
- Declarative  
- Self‑healing  
- Decoupling — каждый компонент инфраструктуры независим от других  

---

## Архитектура

- **API сервер** — входная точка для запуска в Kubernetes  
- **Мастер** — смотрит в API сервер, занимается операционными делами  
- **Кублет** — нода-агент на всех серверах кластера, отправляет данные Docker-демону о контейнерах  
- **Worker-ноды** — машины в кластере, где запускаются приложения; на каждой ноде есть kubelet

---  

## Абстракции Kubernetes (YAML)

### POD
- Минимальная абстракция, в которой запускается приложение  
- Группа контейнеров с общими ресурсами, запускается как единое целое
- В POD может быть несколько контейнеров
- 1 POD = 1 контейнер 
- В любом поде будет +1 контейнер, который называется pause контейнер. Он ничего не делает. Его функция - создать сетевой namespace 
- `kubectl get pod --show-labels`

### ReplicaSet
- Родитель POD, контролирует количество запущенных копий
- Labels - метка  
- Selector - инструкция для репликасет, глядя в него понимает, какие POD его. Понимает он это благодаря метке на подах
- Смотрит на ко-во, но не на обновление

### Deployment
- Родитель ReplicaSet, позволяет обновлять версии приложения  
- Стратегии:
  - `rollingUpdate` — постепенное обновление  
  - `recreate` — одновременная замена всех реплик
- **Probes** - контроль за состоянием приложения во время его жизни, старта, готово ли приложение:
  - **Liveness Probe** — проверяет, жив ли контейнер  
  - **Readiness Probe** — определяет, готов ли контейнер принимать трафик. В случае неудачного выполнения приложение убирается из балансировки    
- **Resources** - содержит лимиты и резервировать запросы:
  - Limits — кол-во ресурсов, которые POD может использовать     
  - Requests — кол-во ресурсов, которые резервируются для POD на ноде (CPU, memory)     
- **Secret** — хранение паролей, токенов, сертификатов  
- Команда отката:
```
kubectl rollout undo deployment name-deployment
```
### ConfigMap
- Хранение конфигурации приложения (например, монтирование томов)

### Service
- Объединение подов с разными IP в единый сервис и распределение трафика  
- Автоматически создаёт объект Endpoints, т.е. сервис создался, выбрал все поды по меткам, и адреса всех этих подов сохрани в Endpoints.

### Ingress
- Управление входящим трафиком
- Это набор правил внутри вашего кластера, предназначенных для того, чтобы входящие подключения могли достичь сервисов (Services) ваших приложений
- Основные задачи, которые решает Ingress:  
  - Задавать внешние URL
  - Балансировать трафик
  - Терминировать SSL
  - Виртуальный хостинг по доменным именам  
- **IngressController** — принимает трафик и направляет его по правилам Ingress

### PV / PVC
- Описывает том, на котором хранятся наши данные (БД)
- PV Provisioner - программа,  котрая умеет взаимодействовать с провайдером (который предоставляет тома с данными) и создавать на этом провайдере том с нужным размером. 
- **StorageClass** - манифест, в котором описывается параметры доступа к провайдеру хранения данных
  
---

## Компоненты кластера

**etcd** — хранилище конфигурации и состояния кластера  
- `etcdctl` — CLI для управления кластером ETCD  
- Требует быстрых дисков

**API server** — центральный компонент Kubernetes, REST API, работает с etcd
- Единственный, кто общается с Etcd
- Работает по REST API
- Авторизация и аутентификация 

**Controller-manager** — набор контроллеров:
- Node controller - контролирует состояние нод 
- Replication controller - контролирует репликасеты
- Endpoints controller
- GarbageCollecor - сборщик мусора (удаляет старые версии репликасет, к примеру)

**Scheduler** — назначает PODы на ноды, учитывая:
- QoS 
- Affinity/anti-Affinity - смотрит на правила, где нужно запсукать приложения
- Requested resources - учитывает лимиты, запросы, зарезервированные ресурсы

**kubelet** — работает на каждой ноде, взаимодействует с Docker daemon
- Единственный компонент, работающий не в Docker
- Отдает команды Docker daemon
- Создает PODы
  
**kube-proxy** — управляет сетевыми правилами на нодах
- Смотрит в Kube-API
- Стоит на всех серверах 
- Фактически реализует Service (ipvs и iptables) 

**Kubespray** - это сценарий для установки Kubernetes 

--- 

## Сеть
Network plugin (Flannel, Calico) 
- Обеспечивает связь между нодами и подами 
- Раздает IP-адреса подам
- Реализует шифрование между нодами 
- Управляет Network Policies

---

## Продвинутые абстракции
### DaemonSet
- Запускает поды на всех нодах кластера  
- Автоматически реагирует на добавление/удаление узлов  
- Количество реплик = количество узлов

### StatefulSet
- Позволяет запускать группу подов (как Deployment)
- Гарантирует:
  - Уникальность и порядок запуска подов  
  - Одноимённые PVC  
- Используется для stateful-приложений (Rabbit, DB, Kafka, Redis)

 ### InitContainers
- Выполняются перед основными контейнерами по порядку  
- Могут использовать те же тома и разные пользователя 

### Headless Service
- `spec.clusterIP: None`
- DNS резолвит не на сервис, а прямо на Endpoints

### Job / CronJob
- **Job** — единоразовая задача с перезапуском до успешного результата  
- **CronJob** — плановые задачи по cron, с параметрами:
  - startingDeadlineSeconds - стремная опция. Работает не так, как описано в доке. Никогда лучше не испоьзовать.
  - concurrencyPolicy - Allow/Forbit запуск экземпляов в одно и тоже время. Использовать Forbit
  - successfulJobsHistoryLimit - кол-во успешных выполненных задач сохраняем 
  - failedJobsHistoryLimit - кол-во неудачных выполненных задач сохраняем 


#### Role Based Access Control (RBAC) 
- ServiceAccount - (в Kubernetes нет юзеров) сущность придумана для того, чтобы приложения Kubernetes работали под ServiceAccount
- Role - описывает, что можно делатьв разделе Kubernetes (правила)
- RoleBinding - в сущности указывается связь ServiceAccount с Role
- ClusterRole - то же самое, что и Role, толко действует на весь кластер 
- ClusterRoleBinding - то же самое, что и RoleBinding, толко действует на весь кластер

---

## Мониторинг и логирование
- На каждой ноде агент собирает данные централизованно
- Управляются агенты из одной точки
- Конфигурируются так же из одной точки 
- Prometheus + Alerting (PromQL; Blackbox & Whitebox мониторинг)  
- Логи: stdout/stderr → Fluent-bit / Fluentd → Loki / Elasticsearch → визуализация (Grafana / Kibana)  
- Tracing, APM, observability

---
## Публикация приложений

### Service типы
Kubernetes Service:  
- ClusterIP - настройка взаимодействий PODов внутри. Не внешне 
- NodePort - публикация внешне (порты от 3000 до 32000)
- LoadBanalcer - публикация внешне, но в облаках 
- ExternalName - можно перенаправлять запросы на другие сервисы (почти не используется)
- ExternalIPs - при создании сервиса на всех серверах кластера создаются по аналогии с Service type: Node Port правило трансляции, который говорит, что трафик, приходящий на адресс ExternalIPs надо отправлять в PODы. Эти правила будут создавны на всех серверах. Это решает админ кластера с задачей балансера. 
- Headless - none 

### Ingress
- IngressController - работает на принципе балансера 
- Манифест, в котором описано правило для IngressController 
- Annotations - возможность передать какую-то специфическую информацию контроллеру или оператору (то, что не влезает в манифест)

### Cert-manager
- Начинался как способ получить сертификат от LetsEncrypt
- Автоматизирует получение SSL/TLS-серификатов от различных удостовуряющих центров (LetsEncrypt)
- Интергрируется с IngressController
- Автоматизирует продление сертификатов 

---

## NameSpace
- Возможность разделить окружение 
- Используется для разделения команд, проектов и русорсов кластера 
- Два одинаковы объекта с одинаковыми именами не могут быть созданы в одном namespace 
- Ресурсы задаются в рамках namespace через ResourceQuota 

## Helm 
- Удобен тем, что при неудачном релизе автоматически выполняется откат, без необходимости вмешательства вручную  
- Позволяет шаблонизировать конфигурации приложения  
- Является пакетным менеджером для Kubernetes  
- Работает на декларативной модели  
- Входит в экосистему CNCF  
- В версии 2 состоит из двух компонентов: Helm и Tiller  
- Поддерживает расширение через плагины  
- Включает важные возможности для реализации CI/CD  
- Поддерживает механизмы отслеживания изменений (`watch`)  
- Обеспечивает функцию отката (`rollback`) 

### Структура пакета Helm
- Содержит шаблоны манифестов Kubernetes  
- Файл с переменными значений (например, `values.yaml`)  
- Метаданные чарта  
- Всё упаковано в архив `.tgz`, называемый чартом (chart)    
Файл `values.yaml` позволяет гибко задавать параметры, которые затем используются в шаблонах, включая `Deployment`.

#### Тестирование релиза 
1. Создаем папку templates/tests 
2. Кладем туда манифесты объектов k8s, которые будут тестить рлиз 
3. Манифесты должны содержать аннотацию helm.sh/hook : test 
4. Запускаем в CI helm test <release name>

#### Хранение данных на примере ceph 
Подключение к подам SC/PVC/PV 
- Storage class - хранит параметры подключения 
- PersistentVolumeClaim - описывает требования к тому 
- PersistentVolume - хранит параметры и стату тома 
- Provisioner - параметр SC, плагин создания томов 

---

## Disaster recovery
Disaster recovery Kubernetes = аварийное восстановление Kubernetes     
Как можно сломать kubernetes?   
- Серверы с k8s сгорели, уборщица выдернула кабель
- Залезли руками в логику k8s, в etcd 
- Джуниор удалил namespace kube-system
- Протухли kubernetes-сертификаты
- Криво обновили версию кластера

#### На что нужно обратить внимание? 
1. Отказоустойчивость сетап kubernetes
  - Кол-во компонентов = кол-ву etcd мастеровых
  - Kubelet, kubeproxy через nginx 
2. Резервирование ingress-controller'a
3. Сертификаты кластера
  - Это те, что лежат в /etc/kubernetes/pki
  - В старых версиях kubernetes сертификаты протухли
  - Пользуйтесь нашим форком Kubespray: https://github.com/southbridgeio/kubespray
  - С версии 1.15 стало легче дить, а с 1.17 совсем легко
4. Регламентные работы (обновление кластера или ПО)
  - Убеждаемся, что у нашего приложения >1 реплик
  - Эвакуируем поды с нужной ноды (kubectl drain)
  - Проводим обновдение
  - Снимаем ноду с обслуживания (kubectl uncordon)
5. Обновление версии кластера:
  - Пробуем на dev-среде/стенде и читаем Changelog!
  - Обновляем основную мастер-ноуды на 1 версию
  - Обновляем остальные мастер-ноды на 1 версию
  - В kubespray для этого есть отдельный сценарий 
6. Бэкапы
  - Манифесты
  - Секреты
  - Сертификаты и настройки узлов
  - Образы контейнеров
  - Содержимое value
  - БД
  - Можно использвать утилиту Hepito Velero
7. Расширенные инструменты k8s (PSP, PDB, RQ..)
  
#### Полезные штуки 
1. PodDisruptionBudgets - ограничиваем кол-во недоступных инстансов приложения
2. Pod Security Policy - контролируем возможности подов (privileged, hostNetwork)
3. PriorityClass - приоритизуем более важные поды над менее важными. MUST HAVE!
4. LimitRange - ставим дефолтные лимиты на ресурсы подов (CPU, RAM) MUST HAVE!
5. ResourceQuota - ограничиваем ресурсы в namespace и кол-во объектов (подов, сикретов)
6. Network Policy - "файервол" внутри кубернетес   

#### Выводы: .
- Иметь описанный регламент по типовым работам
- Размещаться в хорошем ДЦ
- Настроить отказоустойчивый сетап
- Иметь dev-стенд, где экспериментировать 
- Покрыться мониторингами, делать бэкапы 
- Держать руку на пульсе, иметь актуальные знания 
- Пользоваться всеми фишками кубернетес
    
--- 

## Обновление кластера Kubernetes 
Проблемы при обновлении:  
  1. устаревшие версии манифестов 
  2. устаревшие ключи запуска 
  3. измененные опции 
  4. исправленные ошибки 
  5. обновление docker 

#### Порядок обновления: 
  1. Изучаем Change log
  2. Устанавливаем тестовый кластер 
  3. Обновляем тестовый кластер 
  4. Деплоим приложения, проверяем работу
  5. Планируем время обновления 
  6. Делаем бэкапы 
  7. Обновляем прод по одной ноде 
  8. После каждой ноды проверяем работоспособность 

#### Что обновляем:
  - etcd database
  - control plane: API, controller-manager, scheduler + kubelet 
  - kubelet on worker proxy 
  - kube-proxy 
  - kube-flannel 
  - coredns, nodelocaldns
  - ingress-nginx-controller
  - certificate

--- 

## Мониторинг и Логирование
- В Prometheus не нужно хранить долго данные. В TSDB хранят буквально несколько часов
- Алертинги настраиваюися на основе label
- Таргеты (цели) - мы пишем, что нужно искать
- Все запросы пишутся на языке PromQL.

## Логирование в kubernetes
  - Eсть несколько лог-драйверов (плагинов)
  - Логи приложение должно писать в stdout/stderr
  - Настройка логирования производятся в deamon.json
  - Docker CE имеет ограниченный набор плагинов по сранение с Docker EE 

Docker работает в поде. Через stdout/stderr пишутся логи в log-file.log. Агент сборщик забирает данные из log-file.log и отправляет в log backend.  
  
#### Особенности логирования k8s:  
  - Сохранять между деплоями
  - Агрегировать со всех инстансов
  - Обавлять мету 
  - Парсить 
  - На текущий момент нет никакого стандарта 

#### Grafana Loki 
  - Легко устанавливается 
  - Потребляет мало ресурсов 
  - Не требует доп ПО 
  - Используется для хранения TSDB
  - Для визуализации используется Grafana 

#### Язык LogQL
- Elastic + Fluent-bit + Kibana = EFK 
- Fluent-bit потребляет меньше ресурсов (в 40 раз), чем FluentD 
- Fluentd собирает логи со всех подов, все эти логи он отправляет в хранилище (elastcksearch, postgre, kafka), kibana достает все эти логи их хранилища и визуализирует.

--- 

## Требования к разработке приложения в Kubernetes 
#### Логи 
- Куда: stdout/stderr ; сборщик логов ; не в файлик!   
- Формат: json (идеально) ; чтобы можно было парсить ; избегать многострочных   

#### Конфигурация 
- ENV переменные 
- Конфиги: удобный формат, используем configmap (yml)
- Секретную информацию - в секреты  

#### Health check 
- Проверка того, что ваше приложение работает 
- Readiness/Liveness пробы 
- Проверять нижележащие сервисы 
- Статус ответа: 2xx - успешно, 5xx - ошибка 
    
#### Graceful Shutdown 
Корректное (мягкое) завершение работы:
- Учитываем завершение операций (считаем, что для приложения нужно 30 сек для завершения всех операций на очереди, после 45 сек принудительно кильнуть)
- Учитываем завершение сессий 
- Возвращаем адекватный exit-код 
- Учитываем на клиенте (сессии могут порваться и их нужно переустановить)

### Ресурсы 
РЕСУРСЫ НУЖНО ПРОСТАВЛЯТЬ!!!   
Requestы и лимиты 

### Хранение данных
В идеале данные не храним   

#### Cloudnativeness 
Можно использовать возможности Kubernetes:  
  - Запускать воркеры
  - Определять эндпоинты 

Использовать возможности облака 
  - Балансировка 
  - Масштабирование

---

## Infrastructure as a code 

#### Докеризация и CI/CD 
  1. Докеризация решает проблемы повторяемости 
  2. Позволяет значительно упростить и ускорить процесс доставки новых изменений в тестовую среду и в продакшен.
  3. При использовании автоматизированного тестирования позволяет отсеять некоторый процент ошибок на пути в продакшен.
  4. Позволяет хранить историю сборок за какой-то промежуток времени, при необходимости можно откатиться назад.

#### Возможные проблемы: 
  1. Ваше приложение читает настройки из файлов (нужно в переменные окружения)
  2. Приложение пишет в логи только в файл (нужно в stdout/stderr)
  3. Приложение сохраняет важные данные в файлы во время работы (нужно использовать внутренние хранилища)
  4. Приложению нужны права root (нужно, чтобы не нужны были права root)
  5. Нужно четко понимать, какие версии ПО нужны для запука приложения и указывать это в Dockerfile 
  6. Нужно хотя бы примерно понимать, какие аппаратные ресурсы нужны приловению для работы 

#### Автоматизация труда Gitlab:
  - Система контроля версий
  - Встроенный Docker registry 
  - Разделение инстансов - gitlab и gitlab-runner 
  - Поддержка Kubernetes и Auto DevOps 

#### Этапы CI/CD 
  1. build 
  2. test
  3. cleanup 
  4. push 
  5. deploy 

#### Observability
Observability - принципы и техники наблюдения за системой   
- Blackbox мониторинг - закрытая коробка, она у нас есть и она не сломалась  
- Whitebox мониторинг - смотрим внутрь 

#### Что алертить?
- То, что требует действий 
- Обязательно разделение по важности
- Все остальное решает визуализация и регламент ее обработки 
  
#### Логи 
**Логи** - это события 
**Парсить:** Fluent[d|bit]; Logstash 
**Хранить:** ElastckSearch; ClickHouse; Loki
**Строить визуализацию:** Grafana(можно интегрировать с метриками); Kibana 
**Алертить:** Kibana; Grafana; Elastalert

#### Трейсинг 
- Поиск узких мест
- Визуализация пути выполнения работы
- Сбор аналитической информации 

####  Application Performance Monitoring 
Мониторинг + Логи + Трейсинг 

